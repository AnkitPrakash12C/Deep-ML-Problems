import numpy as np

def multi_head_latent_attention(
    X: np.ndarray,
    W_dkv: np.ndarray,
    W_uk: np.ndarray,
    W_uv: np.ndarray,
    W_dq: np.ndarray,
    W_uq: np.ndarray,
    W_o: np.ndarray,
    n_heads: int
) -> tuple:
    """
    Perform Multi-Head Latent Attention (MLA).
    
    Args:
        X: Input tensor of shape (seq_len, d_model)
        W_dkv: Down-projection for KV compression (d_model, d_c_kv)
        W_uk: Up-projection for keys (d_c_kv, d_model)
        W_uv: Up-projection for values (d_c_kv, d_model)
        W_dq: Down-projection for query compression (d_model, d_c_q)
        W_uq: Up-projection for queries (d_c_q, d_model)
        W_o: Output projection (d_model, d_model)
        n_heads: Number of attention heads
    
    Returns:
        Tuple of (output, c_kv) where:
        - output: shape (seq_len, d_model)
        - c_kv: compressed KV latent of shape (seq_len, d_c_kv)
    """
    seq_len, d_model = X.shape
    d_head = d_model // n_heads

    c_kv = X @ W_dkv                 

    K = c_kv @ W_uk                    
    V = c_kv @ W_uv                    

    c_q = X @ W_dq                      
    Q = c_q @ W_uq                      

    def split_heads(x):
        x = x.reshape(seq_len, n_heads, d_head)
        return x.transpose(1, 0, 2)   

    Q = split_heads(Q)
    K = split_heads(K)
    V = split_heads(V)


    outputs = []

    for h in range(n_heads):
        q = Q[h]                        
        k = K[h]
        v = V[h]

        scores = q @ k.T / np.sqrt(d_head)

        scores = scores - np.max(scores, axis=-1, keepdims=True)
        attn = np.exp(scores)
        attn = attn / np.sum(attn, axis=-1, keepdims=True)

        head_output = attn @ v          

        outputs.append(head_output)

    concat = np.concatenate(outputs, axis=-1)  

    output = concat @ W_o

    return output, c_kv
    pass
