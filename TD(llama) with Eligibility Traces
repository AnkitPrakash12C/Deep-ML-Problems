import numpy as np

def td_lambda_prediction(
    episodes: list[list[tuple[int, float]]],
    n_states: int,
    gamma: float,
    lambd: float,
    alpha: float
) -> np.ndarray:
    """
    Estimate state values using TD(λ) with accumulating eligibility traces.
    
    Args:
        episodes: List of episodes. Each episode is a list of (state, reward) tuples.
                  The reward at index i is the reward received AFTER leaving state i.
        n_states: Number of states (states are integers 0 to n_states-1)
        gamma: Discount factor
        lambd: Trace decay parameter (λ)
        alpha: Learning rate
        
    Returns:
        V: Estimated state values as numpy array of shape (n_states,)
    """
    V = np.zeros(n_states)

    for episode in episodes:
        e = np.zeros(n_states)

        for t in range(len(episode)):
            s_t, r_t = episode[t]

            if t + 1 < len(episode):
                s_next, _ = episode[t + 1]
                v_next = V[s_next]
            else:
                v_next = 0.0  

            delta = r_t + gamma * v_next - V[s_t]

            e *= gamma * lambd

            e[s_t] += 1.0

            V += alpha * delta * e

    return V
