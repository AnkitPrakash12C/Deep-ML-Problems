import numpy as np

def sparse_window_attention(Q, K, V, window_size, scale_factor=None):

    seq_len, d_k = Q.shape
    d_v = V.shape[1]

    if scale_factor is None:
        scale_factor = np.sqrt(d_k)

    output = np.zeros((seq_len, d_v))

    for i in range(seq_len):
        start = max(0, i - window_size)
        end = min(seq_len, i + window_size + 1)

        q = Q[i]                    
        k = K[start:end]             
        v = V[start:end]             

        scores = (k @ q) / scale_factor   

        scores -= np.max(scores)
        weights = np.exp(scores)
        weights /= np.sum(weights)

        output[i] = weights @ v

    return output
    pass
